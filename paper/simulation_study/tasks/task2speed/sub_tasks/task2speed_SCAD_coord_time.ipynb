{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T02:04:47.722403Z",
     "start_time": "2022-09-20T02:04:46.820121Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize_scalar\n",
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from scipy.stats import logistic\n",
    "from cupyx.scipy.linalg import toeplitz, block_diag\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "import multiprocessing\n",
    "import cProfile\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # this is just to hide all the warnings\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "import matplotlib.pyplot as plt  # change font globally to Times\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.sans-serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12\n",
    "})\n",
    "\n",
    "os.chdir(sys.path[0])  # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T02:04:47.772559Z",
     "start_time": "2022-09-20T02:04:47.723713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installed CuPy version is: 8.3.0\n",
      "The CUDA device has compute capability of: 75\n"
     ]
    }
   ],
   "source": [
    "print(\"This script gets the running time for coordinate descent running on SCAD-penalized logistic models.\")\n",
    "print(\"The installed CuPy version is:\", cp.__version__)\n",
    "print(\"The CUDA device has compute capability of:\",\n",
    "      cp.cuda.device.get_compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:20.372273Z",
     "start_time": "2022-08-22T20:10:20.362659Z"
    }
   },
   "outputs": [],
   "source": [
    "class tensor_computation:\n",
    "    '''\n",
    "    This is just a fundemental class used for tensor computation;\n",
    "    its main purpose here is to serve as a class which LMM_SCAD_MCP can inherent from later. \n",
    "    '''\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Basic declaration for class. \n",
    "        '''\n",
    "        return \"This is just a fundemental linear algebra computation setup for class creation in the future steps\"\n",
    "\n",
    "    ######################################  some SCAD and MCP things  #######################################\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "        '''\n",
    "        To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "        This function is to be used for calculation involving L1 penalty term later. \n",
    "        '''\n",
    "        return cp.concatenate((cp.array([x[0]]),\n",
    "                               cp.where(\n",
    "                                   cp.abs(x[1:]) > lambda_,\n",
    "                                   x[1:] - cp.sign(x[1:]) * lambda_, 0)))\n",
    "\n",
    "    def soft_thresholding_scalar(self, x, lambda_):\n",
    "        '''\n",
    "        To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "        This function is to be used for calculation involving L1 penalty term later. \n",
    "        '''\n",
    "        return cp.where(cp.abs(x) > lambda_, x - cp.sign(x) * lambda_, 0)\n",
    "\n",
    "    def SCAD(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        To calculate SCAD penalty value;\n",
    "        #x can be a multi-dimensional tensor;\n",
    "        lambda_, a are scalars;\n",
    "        Fan and Li suggests to take a as 3.7 \n",
    "        '''\n",
    "        # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, lambda_ * x,\n",
    "            cp.where(x < a * lambda_,\n",
    "                     (2 * a * lambda_ * x - x**2 - lambda_**2) / (2 * (a - 1)),\n",
    "                     lambda_**2 * (a + 1) / 2))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_grad(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        To calculate the gradient of SCAD wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # here decompose x to sign and its absolute value for easier calculation\n",
    "        sgn = cp.sign(x)\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, lambda_ * sgn,\n",
    "            cp.where(x < a * lambda_, (a * lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                     0))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        To calculate MCP penalty value; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # the function is a function of absolute value of x\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(x <= gamma * lambda_, lambda_ * x - x**2 / (2 * gamma),\n",
    "                        .5 * gamma * lambda_**2)\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_grad(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        To calculate MCP gradient wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        temp = cp.where(\n",
    "            cp.abs(x) < gamma * lambda_,\n",
    "            lambda_ * cp.sign(x) - x / gamma, cp.zeros_like(x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_concave(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        The value of concave part of SCAD penalty; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, 0.,\n",
    "            cp.where(x < a * lambda_,\n",
    "                     (lambda_ * x - (x**2 + lambda_**2) / 2) / (a - 1),\n",
    "                     (a + 1) / 2 * lambda_**2 - lambda_ * x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_concave_grad(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        The gradient of concave part of SCAD penalty wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        sgn = cp.sign(x)\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, 0.,\n",
    "            cp.where(x < a * lambda_, (lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                     -lambda_ * sgn))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_concave(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        The value of concave part of MCP penalty; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # similiar as in MCP\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(x <= gamma * lambda_, -(x**2) / (2 * gamma),\n",
    "                        (gamma * lambda_**2) / 2 - lambda_ * x)\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_concave_grad(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        The gradient of concave part of MCP penalty wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        temp = cp.where(\n",
    "            cp.abs(x) < gamma * lambda_, -x / gamma, -lambda_ * cp.sign(x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Illustrations for SCAD and MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:20.814993Z",
     "start_time": "2022-08-22T20:10:20.373582Z"
    }
   },
   "outputs": [],
   "source": [
    "x = cp.arange(-15.1, 15.2, .1)\n",
    "class_temp = tensor_computation()\n",
    "\n",
    "plt.plot(x[1:-1].get(), class_temp.soft_thresholding(x, lambda_=2)[1:-1].get())\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'soft-thresholding when $\\lambda$=2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:21.692826Z",
     "start_time": "2022-08-22T20:10:20.816459Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].plot(x[1:-1].get(), np.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].plot(x[1:-1].get(), np.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].plot(x[1:-1].get(), np.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].plot(x[1:-1].get(), np.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='penalty')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "# plt.savefig('SCAD_MCP.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.440971Z",
     "start_time": "2022-08-22T20:10:21.694256Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='concave part')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "# plt.savefig('SCAD_MCP_concave.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.637365Z",
     "start_time": "2022-08-22T20:10:22.442363Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='concavity gradient')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.870905Z",
     "start_time": "2022-08-22T20:10:22.638475Z"
    }
   },
   "outputs": [],
   "source": [
    "# To plot the derivatives of SCAD and MCP\n",
    "x[np.abs(x) < 1e-10] = float(\"nan\")\n",
    "markerstyle = {\n",
    "    \"markersize\": 8,\n",
    "    \"markeredgecolor\": \"black\",\n",
    "    \"markerfacecolor\": \"w\",\n",
    "    \"linestyle\": \"none\"\n",
    "}\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='derivative')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "\n",
    "The negative log-likelihood loss for logistic is: \n",
    "\n",
    "$$\\frac{\\left(\\sum_{i}\\log\\left(\\exp\\left(\\mathbf{X}\\boldsymbol{\\beta}\\right)+\\mathbf{1}\\right)_{i}\\right)-\\mathbf{y}^{T}\\mathbf{X}\\boldsymbol{\\beta}}{2N}$$\n",
    "\n",
    "its gradient is then: \n",
    "\n",
    "$$\\frac{\\mathbf{X}^{T}\\left(\\sigma\\left(\\mathbf{X}\\boldsymbol{\\beta}\\right)-\\mathbf{y}\\right)}{2N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.914524Z",
     "start_time": "2022-08-22T20:10:22.872202Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class logistic_SCAD_MCP(tensor_computation):\n",
    "    '''\n",
    "    This class performs SCAD/MCP pruning on linear models.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 design_matrix,\n",
    "                 outcome,\n",
    "                 penalty,\n",
    "                 _lambda,\n",
    "                 a=3.7,\n",
    "                 gamma=2,\n",
    "                 beta_0=\"NOT DECLARED\",\n",
    "                 tol=1e-4,\n",
    "                 maxit=5000,\n",
    "                 L_convex=\"NOT DECLARED\"):\n",
    "        '''\n",
    "        Class constructor:\n",
    "        design_matrix:           the design matrix for the linear models;\n",
    "        outcome:                 the outcome for the linear model;\n",
    "        penalty:                 \"SCAD\" or \"MCP\";\n",
    "        _lambda:                 value for lambda; \n",
    "        a:                       value for a, only used when penalty set to be \"SCAD\";\n",
    "        gamma:                   value for gamma, only used when penalty set to be \"MCP\";\n",
    "        beta_0:                  initial values for beta;\n",
    "        tol:                     tolerance parameter set for beta, this is for the maximum change of beta;\n",
    "        maxit:                   maximum number of iterations allowed;\n",
    "        '''\n",
    "        assert penalty in (\n",
    "            \"SCAD\", \"MCP\"), \"Choose between \\\"SCAD\\\" or \\\"MCP\\\" for penalty\"\n",
    "        assert a > 2, \"SCAD penalty parameter, a, needs to be greater than 2\"\n",
    "        assert gamma > 0, \"MCP penalty parameter, gamma, needs to be positive\"\n",
    "        assert _lambda > 0, \"penalty paramter, lambda_, needs to be positive\"\n",
    "        assert tol > 0, \"tol should be postive\"\n",
    "        assert maxit > 0, \"maxit is the maximum iteration allowed; which needs to be positive\"\n",
    "        # Construct self\n",
    "        self.X = design_matrix\n",
    "        self.y = outcome\n",
    "        self.N = self.X.shape[0]\n",
    "        #        cov = (self.y - cp.mean(self.y))@(self.X - cp.mean(self.X, 0).reshape(1,-1))\n",
    "        if type(beta_0) == str:\n",
    "            self.beta = cp.zeros(self.X.shape[1])  #cp.sign(cov)\n",
    "        else:\n",
    "            self.beta = beta_0\n",
    "        # add design matrix column for the intercept, if it's not there already\n",
    "        if cp.any(\n",
    "                cp.all(self.X == self.X[0, :], 0)\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            pass\n",
    "        else:\n",
    "            intercept_design = cp.ones(self.N).reshape(self.N, 1)\n",
    "            self.X = cp.concatenate((intercept_design, self.X), 1)\n",
    "            if type(beta_0) == str:\n",
    "                self.beta = cp.concatenate((cp.array([0.]), self.beta))\n",
    "        # passing other parameters\n",
    "        self.tol = tol\n",
    "        self.maxit = maxit\n",
    "        self._lambda = _lambda\n",
    "        self.penalty = penalty\n",
    "        #        if penalty == \"SCAD\":\n",
    "        self.a = a\n",
    "        #        else:\n",
    "        self.gamma = gamma\n",
    "        self.p = self.X.shape[\n",
    "            1]  # so here p includes the intercept design matrix column\n",
    "        self.smooth_grad = cp.ones(self.p)\n",
    "        self.beta_ag = self.beta.copy()\n",
    "        self.beta_md = self.beta.copy()\n",
    "        self.k = 0\n",
    "        self.FISTA_k = 0\n",
    "        self.converged = False\n",
    "        self.obj_value = []\n",
    "        self.obj_value_ORIGINAL = []\n",
    "        self.obj_value_AG = []\n",
    "        self.obj_coord_value = []\n",
    "        self.opt_alpha = 1\n",
    "        if type(L_convex) == str:\n",
    "            self.L_convex = 1 / (8 * self.N) * cp.max(\n",
    "                cp.linalg.eigh(self.X @ self.X.T)[0]).item()\n",
    "        else:\n",
    "            self.L_convex = L_convex\n",
    "        self.FISTA_beta = cp.empty_like(self.beta)\n",
    "\n",
    "    def update_smooth_grad_convex(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth convex objective component.\n",
    "        '''\n",
    "        self.smooth_grad = (self.X.T @ (np.tanh(self.X @ self.beta_md / 2) / 2\n",
    "                                        - self.y + .5)) / (2 * self.N)\n",
    "\n",
    "    def update_smooth_grad_SCAD(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth objective component for SCAD penalty.\n",
    "        '''\n",
    "        self.update_smooth_grad_convex()\n",
    "        self.smooth_grad += self.SCAD_concave_grad(self.beta_md,\n",
    "                                                   lambda_=self._lambda,\n",
    "                                                   a=self.a)\n",
    "\n",
    "    def update_smooth_grad_MCP(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth objective component for MCP penalty.\n",
    "        '''\n",
    "        self.update_smooth_grad_convex()\n",
    "        self.smooth_grad += self.MCP_concave_grad(self.beta_md,\n",
    "                                                  lambda_=self._lambda,\n",
    "                                                  gamma=self.gamma)\n",
    "\n",
    "    def eval_obj_SCAD(self, x_temp, obj_value_name):\n",
    "        '''\n",
    "        evaluate value of the objective function.\n",
    "        '''\n",
    "        obj_value_name += [\n",
    "            (-self.y.T @ self.X @ self.beta_md +\n",
    "             cp.sum(cp.logaddexp(self.X @ self.beta_md, 0))) / (2 * self.N) +\n",
    "            cp.sum(self.SCAD(self.beta_md, self._lambda, self.a))\n",
    "        ]\n",
    "\n",
    "    def eval_obj_MCP(self, x_temp, obj_value_name):\n",
    "        '''\n",
    "        evaluate value of the objective function.\n",
    "        '''\n",
    "        obj_value_name += [\n",
    "            (-self.y.T @ self.X @ self.beta_md +\n",
    "             cp.sum(cp.logaddexp(self.X @ self.beta_md, 0))) / (2 * self.N) +\n",
    "            cp.sum(self.MCP(self.beta_md, self._lambda, self.a))\n",
    "        ]\n",
    "\n",
    "    def UAG_logistic_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                    )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    #self.opt_alpha = 2/(self.k+1) #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                    )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md - self.beta_ag)) /\n",
    "                        self.opt_beta < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def UAG_restarting_logistic_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        self.old_speed_norm = 0.\n",
    "        self.speed_norm = 1.\n",
    "        self.restart_k = 0\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    if self.old_speed_norm > self.speed_norm and self.k - self.restart_k >= 3:\n",
    "                        self.opt_alpha = 1.\n",
    "                        self.restart_k = self.k\n",
    "                        print(\"restarting at iteration:\", self.restart_k)\n",
    "                    else:\n",
    "                        pass\n",
    "                        #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        self.opt_alpha = 2 / (\n",
    "                            1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                        )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md_old = self.beta_md.copy()\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.old_speed_norm = self.speed_norm\n",
    "                    self.speed_norm = cp.linalg.norm(self.beta_md -\n",
    "                                                     self.beta_md_old)\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    if self.old_speed_norm > self.speed_norm and self.k - self.restart_k > 3:\n",
    "                        self.opt_alpha = 1.\n",
    "                        self.restart_k = self.k\n",
    "                        print(\"restarting at iteration:\", self.restart_k)\n",
    "                    else:\n",
    "                        pass\n",
    "                        #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        self.opt_alpha = 2 / (\n",
    "                            1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                        )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md_old = self.beta_md.copy()\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.old_speed_norm = self.speed_norm\n",
    "                    self.speed_norm = cp.linalg.norm(self.beta_md -\n",
    "                                                     self.beta_md_old)\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def FISTA_logistic_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out FISTA procedure to find out l1 penalized minimizer.\n",
    "        '''\n",
    "        FISTA_t_new = 1.\n",
    "        FISTA_converged = False\n",
    "        x_new, x_old = cp.empty_like(self.beta_md), cp.empty_like(\n",
    "            self.beta_md\n",
    "        )  # beta_md here is y; it plays a very different role here!\n",
    "        self.update_smooth_grad_convex()\n",
    "        x_new = self.soft_thresholding(\n",
    "            self.beta_md - self.smooth_grad / self.L_convex,\n",
    "            self._lambda / self.L_convex)\n",
    "        if self.penalty == \"SCAD\":\n",
    "            self.eval_obj_SCAD(x_new, self.obj_value)\n",
    "        else:\n",
    "            self.eval_obj_MCP(x_new, self.obj_value)\n",
    "        while (not FISTA_converged) and self.FISTA_k <= self.maxit:\n",
    "            self.FISTA_k += 1\n",
    "            self.update_smooth_grad_convex()\n",
    "            x_old = x_new.copy()\n",
    "            x_new = self.soft_thresholding(\n",
    "                self.beta_md - self.smooth_grad / self.L_convex,\n",
    "                self._lambda / self.L_convex)\n",
    "            FISTA_t_old = FISTA_t_new\n",
    "            FISTA_t_new = (1 + cp.sqrt(1 + 4 * FISTA_t_new**2)) / 2\n",
    "            diff_temp = x_new - x_old\n",
    "            self.beta_md = x_new + (FISTA_t_old - 1) / FISTA_t_new * diff_temp\n",
    "            FISTA_converged = cp.all(\n",
    "                cp.max(cp.abs(diff_temp)) < self.tol\n",
    "            ).item(\n",
    "            ) and self.FISTA_k != 1  # since when FISTA_k=1, x_new and x_old are the same\n",
    "            if self.penalty == \"SCAD\":\n",
    "                self.eval_obj_SCAD(x_new, self.obj_value)\n",
    "            else:\n",
    "                self.eval_obj_MCP(x_new, self.obj_value)\n",
    "        self.FISTA_beta = x_new  # because we used self.beta_md as y all the time, now it should be fixed\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "\n",
    "    def Two_step_FISTA_UAG(self):\n",
    "        '''\n",
    "        Carry out the two step combining FISTA and Ghadimi's AG.\n",
    "        '''\n",
    "        self.FISTA_logistic_SCAD_MCP()\n",
    "        self.k = self.FISTA_k  # so FISTA iterations will also count into the number of iterations\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "        return self.UAG_logistic_SCAD_MCP()\n",
    "\n",
    "    def Two_step_FISTA_ISTA(self):\n",
    "        '''\n",
    "        Carry out the two step combining FISTA and ISTA.\n",
    "        '''\n",
    "        self.FISTA_logistic_SCAD_MCP()\n",
    "        self.k = self.FISTA_k  # so FISTA iterations will also count into the number of iterations\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "        return self.vanilla_proximal()\n",
    "\n",
    "    def Two_step_FISTA_Ghadimi(self):\n",
    "        '''\n",
    "        Carry out the two step combining FISTA and the suggested parameter settings.\n",
    "        '''\n",
    "        self.FISTA_logistic_SCAD_MCP()\n",
    "        self.k = self.FISTA_k  # so FISTA iterations will also count into the number of iterations\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "        return self.UAG_logistic_SCAD_MCP_Ghadimi_parameter()\n",
    "\n",
    "    def UAG_logistic_SCAD_MCP_Ghadimi_parameter(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .5 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        self.k + 1\n",
    "                    )  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_lambda = self.k / 2 * self.opt_beta  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_alpha = 2/(1+cp.sqrt(1+4/self.opt_alpha**2)) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    #self.opt_lambda = self.opt_beta/self.opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.opt_beta = .5 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        self.k + 1\n",
    "                    )  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_lambda = self.k / 2 * self.opt_beta  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_alpha = 2/(1+cp.sqrt(1+4/self.opt_alpha**2)) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    #self.opt_lambda = self.opt_beta/self.opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md - self.beta_ag)) /\n",
    "                        self.opt_beta < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def vanilla_proximal(self):\n",
    "        '''\n",
    "        Carry out optimization using vanilla gradient descent.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.vanilla_stepsize = 1 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            self.old_beta = self.beta_md - 10.\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta_md = self.soft_thresholding(\n",
    "                        self.beta_md -\n",
    "                        self.vanilla_stepsize * self.smooth_grad,\n",
    "                        self.vanilla_stepsize * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md -\n",
    "                                      self.old_beta)) < self.tol).item()\n",
    "                    self.old_beta = self.beta_md.copy()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.vanilla_stepsize = 1 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            self.old_beta = self.beta_md - 10.\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta_md = self.soft_thresholding(\n",
    "                        self.beta_md -\n",
    "                        self.vanilla_stepsize * self.smooth_grad,\n",
    "                        self.vanilla_stepsize * self._lambda)\n",
    "                    self.converged = (cp.max(\n",
    "                        cp.abs(self.beta_md - self.old_beta)) < self.tol)\n",
    "                    self.old_beta = self.beta_md.copy()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def coordinate_descent(self):\n",
    "        _Xbeta = self.X @ self.beta_md\n",
    "        while not self.converged:\n",
    "            self.k += 1\n",
    "            if self.k <= self.maxit:\n",
    "                self.beta_md_old = self.beta_md.copy()\n",
    "                if self.penalty == \"SCAD\":\n",
    "                    for j in range(self.p):\n",
    "                        pi = np.tanh(_Xbeta / 2) / 2 + .5\n",
    "                        W = pi * (1 - pi)\n",
    "                        v_j = 1 / self.N * (self.X[:, j] * W) @ self.X[:, j]\n",
    "                        z_j = 1 / self.N * self.X[:, j] @ (\n",
    "                            self.y - pi) + v_j * self.beta_md[j]\n",
    "                        if j != 0:\n",
    "                            _Xbeta -= self.X[:, j] * self.beta_md[j]\n",
    "                            self.beta_md[j] = cp.where(\n",
    "                                cp.abs(z_j) <= self._lambda * (v_j + 1.),\n",
    "                                self.soft_thresholding_scalar(\n",
    "                                    z_j, self._lambda) / v_j,\n",
    "                                cp.where(\n",
    "                                    cp.abs(z_j) <= self._lambda * v_j * self.a,\n",
    "                                    self.soft_thresholding_scalar(\n",
    "                                        z_j, self.a * self._lambda /\n",
    "                                        (self.a - 1.)) /\n",
    "                                    (v_j - 1. / (self.a - 1)), z_j / v_j))\n",
    "                            _Xbeta += self.X[:, j] * self.beta_md[j]\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_coord_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    for j in range(self.p):\n",
    "                        pi = np.tanh(_Xbeta / 2) / 2 + .5\n",
    "                        W = pi * (1 - pi)\n",
    "                        v_j = 1 / self.N * (self.X[:, j] * W) @ self.X[:, j]\n",
    "                        z_j = 1 / self.N * self.X[:, j] @ (\n",
    "                            self.y - pi) + v_j * self.beta_md[j]\n",
    "                        if j != 0:\n",
    "                            _Xbeta -= self.X[:, j] * self.beta_md[j]\n",
    "                            self.beta_md[j] = cp.where(\n",
    "                                cp.abs(z_j) <= self._lambda * self.gamma * v_j,\n",
    "                                self.soft_thresholding_scalar(\n",
    "                                    z_j, self._lambda) /\n",
    "                                (v_j - 1. / self.gamma), z_j / v_j)\n",
    "                            _Xbeta += self.X[:, j] * self.beta_md[j]\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_coord_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                self.converged = (cp.max(\n",
    "                    cp.abs(self.beta_md - self.beta_md_old)) < self.tol)\n",
    "            else:\n",
    "                break\n",
    "        return self.report_results()\n",
    "\n",
    "    def report_results(self):\n",
    "        '''\n",
    "        A results reporting tool.\n",
    "        '''\n",
    "        #self.beta_md[cp.abs(self.beta_md)<self.tol] = 0 # for those estimates below tolerance parameter, set them to 0\n",
    "        self.estimates_constructor = collections.namedtuple(\n",
    "            'Estimates', [\n",
    "                'beta_est', 'converged', 'num_of_iterations', 'obj_values',\n",
    "                'obj_values_orignal', 'obj_values_AG', 'obj_coord_values',\n",
    "                'FISTA_estimates'\n",
    "            ])\n",
    "        results = self.estimates_constructor(self.beta_md, self.converged,\n",
    "                                             self.k, self.obj_value,\n",
    "                                             self.obj_value_ORIGINAL,\n",
    "                                             self.obj_value_AG,\n",
    "                                             self.obj_coord_value,\n",
    "                                             self.FISTA_beta)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong rule implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.936625Z",
     "start_time": "2022-08-22T20:10:22.916403Z"
    }
   },
   "outputs": [],
   "source": [
    "def lambda_max_logistic(X, y):\n",
    "    X_temp = X.copy()\n",
    "    X_temp = X_temp[:, 1:]\n",
    "    X_temp -= cp.mean(X_temp, 0).reshape(1, -1)\n",
    "    X_temp /= cp.std(X_temp, 0)\n",
    "    grad_at_0 = cp.abs((y - cp.mean(y)) @ X_temp / (2 * len(y)))  #  or -.5???\n",
    "    lambda_max = cp.max(grad_at_0[1:])\n",
    "    return lambda_max\n",
    "\n",
    "\n",
    "def strong_rule_seq_logistic(X, y, beta_old, lambda_new, lambda_old):\n",
    "    # suppose that X is already standardized to make it faster, and X has intercept column\n",
    "    X_temp = X.copy()\n",
    "    X_temp = X_temp[:, 1:]\n",
    "    #     X_temp -= cp.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= cp.std(X_temp,0)\n",
    "    grad = cp.abs(\n",
    "        (y - np.tanh(X @ beta_old / 2) / 2 - .5) @ X_temp / (2 * len(y)))\n",
    "    eliminated = (grad < 2 * lambda_new - lambda_old\n",
    "                  )  # True means the value gets eliminated\n",
    "    eliminated = cp.concatenate(\n",
    "        (cp.array([False]),\n",
    "         eliminated))  # because intercept coefficient is not penalized\n",
    "    return eliminated\n",
    "\n",
    "\n",
    "def fit_logistic(X, y, lambda_seq, penalty, a=3.7, gamma=3., tol=1e-5):\n",
    "    '''\n",
    "    A function to fit SCAD/MCP penalized logistic with given lambda_seq (in a decreasing order), under strong rules; with X being standardized automatically (no intercept column); lambda_max will be calculated and added at the begining of lambda sequence.\n",
    "    '''\n",
    "    X_temp = X.copy()\n",
    "    y_temp = y.copy()\n",
    "    lambda_seq_temp = lambda_seq.copy()\n",
    "    X_temp[:, 1:] -= cp.mean(X[:, 1:], 0).reshape(1, -1)\n",
    "    X_temp[:, 1:] /= cp.std(X[:, 1:], 0)\n",
    "    beta_est = cp.zeros((len(lambda_seq_temp) + 1, X.shape[1]))\n",
    "    lambda_seq_temp = cp.concatenate(\n",
    "        (cp.array([lambda_max_logistic(X=X_temp, y=y_temp)]), lambda_seq_temp))\n",
    "    elim = cp.array([False] + [True] * (X.shape[1] - 1))\n",
    "    for i in cp.arange(len(lambda_seq_temp) - 1):\n",
    "        elim_temp = strong_rule_seq_logistic(X_temp,\n",
    "                                             y_temp,\n",
    "                                             beta_old=beta_est[i, :],\n",
    "                                             lambda_new=lambda_seq_temp[i + 1],\n",
    "                                             lambda_old=lambda_seq_temp[i])\n",
    "        elim = cp.logical_and(elim, elim_temp) if i > 0 else cp.array(\n",
    "            [False] + [True] * (X.shape[1] - 1)\n",
    "        )  # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        temp_beta = beta_est[i, :]\n",
    "        cls = logistic_SCAD_MCP(\n",
    "            design_matrix=X_temp[:, cp.invert(elim)],\n",
    "            outcome=y_temp,\n",
    "            penalty=penalty,\n",
    "            _lambda=lambda_seq_temp[i + 1],  # .6 works\n",
    "            a=a,\n",
    "            gamma=gamma,\n",
    "            beta_0=temp_beta[cp.invert(elim)],\n",
    "            tol=tol,\n",
    "            maxit=500)\n",
    "        beta_temp = cp.zeros(X.shape[1])\n",
    "        beta_temp[cp.invert(elim)] = cls.UAG_logistic_SCAD_MCP()[0]\n",
    "        beta_est[i + 1, :] = beta_temp\n",
    "    beta_est[:, 1:] /= (cp.std(X[:, 1:], 0).reshape(1, -1))\n",
    "    return beta_est\n",
    "\n",
    "\n",
    "def fit_logistic_coord(X, y, lambda_seq, penalty, a=3.7, gamma=3., tol=1e-5):\n",
    "    '''\n",
    "    A function to fit SCAD/MCP penalized logistic with given lambda_seq (in a decreasing order), under strong rules; with X being standardized automatically (no intercept column); lambda_max will be calculated and added at the begining of lambda sequence.\n",
    "    '''\n",
    "    X_temp = X.copy()\n",
    "    y_temp = y.copy()\n",
    "    lambda_seq_temp = lambda_seq.copy()\n",
    "    X_temp[:, 1:] -= cp.mean(X[:, 1:], 0).reshape(1, -1)\n",
    "    X_temp[:, 1:] /= cp.std(X[:, 1:], 0)\n",
    "    beta_est = cp.zeros((len(lambda_seq_temp) + 1, X.shape[1]))\n",
    "    lambda_seq_temp = cp.concatenate(\n",
    "        (cp.array([lambda_max_logistic(X=X_temp, y=y_temp)]), lambda_seq_temp))\n",
    "    elim = cp.array([False] + [True] * (X.shape[1] - 1))\n",
    "    for i in cp.arange(len(lambda_seq_temp) - 1):\n",
    "        elim_temp = strong_rule_seq_logistic(X_temp,\n",
    "                                             y_temp,\n",
    "                                             beta_old=beta_est[i, :],\n",
    "                                             lambda_new=lambda_seq_temp[i + 1],\n",
    "                                             lambda_old=lambda_seq_temp[i])\n",
    "        elim = cp.logical_and(elim, elim_temp) if i > 0 else cp.array(\n",
    "            [False] + [True] * (X.shape[1] - 1)\n",
    "        )  # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        temp_beta = beta_est[i, :]\n",
    "        cls = logistic_SCAD_MCP(\n",
    "            design_matrix=X_temp[:, cp.invert(elim)],\n",
    "            outcome=y_temp,\n",
    "            penalty=penalty,\n",
    "            _lambda=lambda_seq_temp[i + 1],  # .6 works\n",
    "            a=a,\n",
    "            gamma=gamma,\n",
    "            beta_0=temp_beta[cp.invert(elim)],\n",
    "            tol=tol,\n",
    "            maxit=500,\n",
    "            L_convex=1.\n",
    "        )  # for coordinate descent, L Lipschitz constant is not needed to compute\n",
    "        beta_temp = cp.zeros(X.shape[1])\n",
    "        beta_temp[cp.invert(elim)] = cls.coordinate_descent()[0]\n",
    "        beta_est[i + 1, :] = beta_temp\n",
    "    beta_est[:, 1:] /= (cp.std(X[:, 1:], 0).reshape(1, -1))\n",
    "    return beta_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some simulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison of number of iterations required for difference of objective values from optimal values to reach $e^{-4}$, with SNR=$5$, Toeplitz being $0.1,0.5,0.9$, and N=$200,500,1000,3000$, p=$2050$ with each $10$ coefficients simulated from $N(.5,1),N(.5,1),N(-.5,1),N(-.5,1),N(1,1)$, sparsely located in the array, with $500$ zeros in-between \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:10:22.955614Z",
     "start_time": "2022-08-22T20:10:22.937632Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulator(seed,\n",
    "              SNR,\n",
    "              Toeplitz,\n",
    "              N,\n",
    "              penalty,\n",
    "              _lambda,\n",
    "              a=3.7,\n",
    "              gamma=2.,\n",
    "              target=-7):\n",
    "    cp.random.seed(seed)\n",
    "    true_beta = cp.array(\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(1, 1, 10).tolist())\n",
    "    X_cov = toeplitz(Toeplitz**cp.arange(2050))\n",
    "    mean = cp.zeros(true_beta.shape[0])\n",
    "    X = cp.random.multivariate_normal(mean, X_cov, N)\n",
    "    X -= cp.mean(X, 0).reshape(1, -1)\n",
    "    X /= cp.std(X, 0)\n",
    "    intercept_design_column = cp.ones(N).reshape(N, 1)\n",
    "    X_sim = cp.concatenate((intercept_design_column, X), 1)\n",
    "    true_sigma_sim = cp.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "    true_beta_intercept = cp.concatenate((cp.array([0.5]), true_beta))\n",
    "    signal = X_sim @ true_beta_intercept + cp.random.normal(\n",
    "        0, true_sigma_sim, N)\n",
    "    y_sim = cp.random.binomial(1, cp.tanh(signal / 2) / 2 + .5)\n",
    "\n",
    "    cls = logistic_SCAD_MCP(design_matrix=X_sim,\n",
    "                            outcome=y_sim,\n",
    "                            penalty=penalty,\n",
    "                            _lambda=_lambda,\n",
    "                            a=a,\n",
    "                            gamma=gamma,\n",
    "                            beta_0=\"NOT DECLARED\",\n",
    "                            tol=1e-4,\n",
    "                            maxit=5000)\n",
    "    obj_val_AG = cp.array(cls.UAG_logistic_SCAD_MCP()[3])\n",
    "    s = \"\"\"cls = logistic_SCAD_MCP(design_matrix = X_sim,outcome = y_sim,penalty = penalty,_lambda = _lambda,a=a,gamma=gamma,beta_0=\"NOT DECLARED\",tol=1e-4,maxit=5000);cls.UAG_logistic_SCAD_MCP()\"\"\"\n",
    "    imports_and_vars = globals()\n",
    "    imports_and_vars.update(locals())\n",
    "    #     _, AG_time = timeit.Timer(stmt=s, globals=imports_and_vars).autorange()\n",
    "\n",
    "    cls = logistic_SCAD_MCP(design_matrix=X_sim,\n",
    "                            outcome=y_sim,\n",
    "                            penalty=penalty,\n",
    "                            _lambda=_lambda,\n",
    "                            a=a,\n",
    "                            gamma=gamma,\n",
    "                            beta_0=\"NOT DECLARED\",\n",
    "                            tol=1e-4,\n",
    "                            maxit=5000)\n",
    "    obj_val_ISTA = cp.array(cls.vanilla_proximal()[3])\n",
    "    s = \"\"\"cls = logistic_SCAD_MCP(design_matrix = X_sim,outcome = y_sim,penalty = penalty,_lambda = _lambda,a=a,gamma=gamma,beta_0=\"NOT DECLARED\",tol=1e-4,maxit=5000);cls.vanilla_proximal()\"\"\"\n",
    "    imports_and_vars = globals()\n",
    "    imports_and_vars.update(locals())\n",
    "    #     _, ISTA_time = timeit.Timer(stmt=s, globals=imports_and_vars).autorange()\n",
    "\n",
    "    cls = logistic_SCAD_MCP(design_matrix=X_sim,\n",
    "                            outcome=y_sim,\n",
    "                            penalty=penalty,\n",
    "                            _lambda=_lambda,\n",
    "                            a=a,\n",
    "                            gamma=gamma,\n",
    "                            beta_0=\"NOT DECLARED\",\n",
    "                            tol=1e-4,\n",
    "                            maxit=5000)\n",
    "    obj_val_Ghadimi = cp.array(\n",
    "        cls.UAG_logistic_SCAD_MCP_Ghadimi_parameter()[3])\n",
    "\n",
    "    s = \"\"\"cls = logistic_SCAD_MCP(design_matrix = X_sim,outcome = y_sim,penalty = penalty,_lambda = _lambda,a=a,gamma=gamma,beta_0=\"NOT DECLARED\",tol=1e-4,maxit=5000,L_convex=1.);cls.coordinate_descent()\"\"\"\n",
    "    imports_and_vars = globals()\n",
    "    imports_and_vars.update(locals())\n",
    "    #     _, coord_time = timeit.Timer(stmt=s, globals=imports_and_vars).autorange()\n",
    "\n",
    "    obj_min_val = cp.min(\n",
    "        cp.array([\n",
    "            cp.min(obj_val_AG),\n",
    "            cp.min(obj_val_ISTA),\n",
    "            cp.min(obj_val_Ghadimi)\n",
    "        ]))\n",
    "\n",
    "    obj_val_AG -= obj_min_val\n",
    "    obj_val_ISTA -= obj_min_val\n",
    "    obj_val_Ghadimi -= obj_min_val\n",
    "\n",
    "    #     obj_val_AG, obj_val_ISTA, obj_val_Ghadimi = cp.log(obj_val_AG), cp.log(obj_val_ISTA), cp.log(obj_val_Ghadimi)\n",
    "    #     target_min = cp.min(cp.array([cp.min(obj_val_AG),cp.min(obj_val_ISTA),cp.min(obj_val_Ghadimi)]))\n",
    "    results = cp.array([cp.inf] * 3)\n",
    "    if cp.any(obj_val_AG <= target) == True:\n",
    "        results[0] = cp.min(cp.where(obj_val_AG <= target)[0])\n",
    "    if cp.any(obj_val_ISTA <= target) == True:\n",
    "        results[1] = cp.min(cp.where(obj_val_ISTA <= target)[0])\n",
    "    if cp.any(obj_val_Ghadimi <= target) == True:\n",
    "        results[2] = cp.min(cp.where(obj_val_Ghadimi <= target)[0])\n",
    "\n",
    "\n",
    "#     results[3] = AG_time\n",
    "#     results[4] = ISTA_time\n",
    "#     results[5] = coord_time\n",
    "# returns number of iterations for AG, ISTA, Ghadimi settings\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T15:02:37.711679Z",
     "start_time": "2022-09-24T15:02:37.697171Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulator_compute_time_AG(seed,\n",
    "                              SNR,\n",
    "                              Toeplitz,\n",
    "                              N,\n",
    "                              penalty,\n",
    "                              _lambda,\n",
    "                              a=3.7,\n",
    "                              gamma=2.,\n",
    "                              target=-7):\n",
    "    cp.random.seed(seed)\n",
    "    true_beta = cp.array(\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(1, 1, 10).tolist())\n",
    "    X_cov = toeplitz(Toeplitz**cp.arange(2050))\n",
    "    mean = cp.zeros(true_beta.shape[0])\n",
    "    X = cp.random.multivariate_normal(mean, X_cov, N)\n",
    "    X -= cp.mean(X, 0).reshape(1, -1)\n",
    "    X /= cp.std(X, 0)\n",
    "    intercept_design_column = cp.ones(N).reshape(N, 1)\n",
    "    X_sim = cp.concatenate((intercept_design_column, X), 1)\n",
    "    true_sigma_sim = cp.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "    true_beta_intercept = cp.concatenate((cp.array([0.5]), true_beta))\n",
    "    signal = X_sim @ true_beta_intercept + cp.random.normal(\n",
    "        0, true_sigma_sim, N)\n",
    "    y_sim = cp.random.binomial(1, cp.tanh(signal / 2) / 2 + .5)\n",
    "\n",
    "    s = \"\"\"cls = logistic_SCAD_MCP(design_matrix = X_sim,outcome = y_sim,penalty = penalty,_lambda = _lambda,a=a,gamma=gamma,beta_0=\"NOT DECLARED\",tol=1e-4,maxit=5000);cls.UAG_logistic_SCAD_MCP()\"\"\"\n",
    "    imports_and_vars = globals()\n",
    "    imports_and_vars.update(locals())\n",
    "    _, AG_time = timeit.Timer(stmt=s, globals=imports_and_vars).autorange()\n",
    "\n",
    "    results = cp.array([cp.inf])\n",
    "    results[0] = AG_time\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T15:02:38.445376Z",
     "start_time": "2022-09-24T15:02:38.428051Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulator_compute_time_coord(seed,\n",
    "                                 SNR,\n",
    "                                 Toeplitz,\n",
    "                                 N,\n",
    "                                 penalty,\n",
    "                                 _lambda,\n",
    "                                 a=3.7,\n",
    "                                 gamma=2.,\n",
    "                                 target=-7):\n",
    "    cp.random.seed(seed)\n",
    "    true_beta = cp.array(\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(-.5, 1, 10).tolist() + [0.] * 500 +\n",
    "        cp.random.normal(1, 1, 10).tolist())\n",
    "    X_cov = toeplitz(Toeplitz**cp.arange(2050))\n",
    "    mean = cp.zeros(true_beta.shape[0])\n",
    "    X = cp.random.multivariate_normal(mean, X_cov, N)\n",
    "    X -= cp.mean(X, 0).reshape(1, -1)\n",
    "    X /= cp.std(X, 0)\n",
    "    intercept_design_column = cp.ones(N).reshape(N, 1)\n",
    "    X_sim = cp.concatenate((intercept_design_column, X), 1)\n",
    "    true_sigma_sim = cp.sqrt(true_beta.T @ X_cov @ true_beta / SNR)\n",
    "    true_beta_intercept = cp.concatenate((cp.array([0.5]), true_beta))\n",
    "    signal = X_sim @ true_beta_intercept + cp.random.normal(\n",
    "        0, true_sigma_sim, N)\n",
    "    y_sim = cp.random.binomial(1, cp.tanh(signal / 2) / 2 + .5)\n",
    "\n",
    "    s = \"\"\"cls = logistic_SCAD_MCP(design_matrix = X_sim,outcome = y_sim,penalty = penalty,_lambda = _lambda,a=a,gamma=gamma,beta_0=\"NOT DECLARED\",tol=1e-4,maxit=5000,L_convex=1.);cls.coordinate_descent()\"\"\"\n",
    "    imports_and_vars = globals()\n",
    "    imports_and_vars.update(locals())\n",
    "    _, coord_time = timeit.Timer(stmt=s, globals=imports_and_vars).autorange()\n",
    "\n",
    "    results = cp.array([cp.inf])\n",
    "    results[0] = coord_time\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCAD `SCAD_sim_results.npy`, `SCAD_sim_results_AG_time.npy`, `SCAD_sim_results_coord_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCAD_sim_results_coord_time = cp.zeros((3, 4, 100, 1))\n",
    "\n",
    "for i, j, seed in itertools.product(range(3), range(4), range(100)):\n",
    "    SCAD_sim_results_coord_time[i, j, seed, :] = simulator_compute_time_coord(\n",
    "        seed=seed,\n",
    "        SNR=5.,\n",
    "        Toeplitz=[0.1, 0.5, 0.9][i],\n",
    "        N=[200, 500, 1000, 3000][j],\n",
    "        penalty=\"SCAD\",\n",
    "        _lambda=.02,\n",
    "        a=3.7,\n",
    "        gamma=2.,\n",
    "        target=cp.exp(-4))\n",
    "\n",
    "cp.save(\"SCAD_sim_results_coord_time\", SCAD_sim_results_coord_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
