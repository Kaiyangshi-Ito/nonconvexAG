{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the class fundementals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:51.528124Z",
     "start_time": "2022-08-22T20:06:50.551952Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.markers as markers\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from scipy.stats import logistic\n",
    "from cupyx.scipy.linalg import toeplitz, block_diag\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "import multiprocessing\n",
    "import cProfile\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # this is just to hide all the warnings\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "import matplotlib.pyplot as plt  # change font globally to Times\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.sans-serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12\n",
    "})\n",
    "\n",
    "os.chdir(sys.path[0])  # ensure working direcotry is set same as the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:51.531723Z",
     "start_time": "2022-08-22T20:06:51.529426Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The installed CuPy version is:\", cp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:51.553107Z",
     "start_time": "2022-08-22T20:06:51.532734Z"
    }
   },
   "outputs": [],
   "source": [
    "class tensor_computation:\n",
    "    '''\n",
    "    This is just a fundemental class used for tensor computation;\n",
    "    its main purpose here is to serve as a class which LMM_SCAD_MCP can inherent from later. \n",
    "    '''\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Basic declaration for class. \n",
    "        '''\n",
    "        return \"This is just a fundemental linear algebra computation setup for class creation in the future steps\"\n",
    "\n",
    "    ######################################  some SCAD and MCP things  #######################################\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "        '''\n",
    "        To calculate soft-thresholding mapping of a given ONE-DIMENSIONAL tensor, BESIDES THE FIRST TERM (so beta_0 will not be penalized). \n",
    "        This function is to be used for calculation involving L1 penalty term later. \n",
    "        '''\n",
    "        return cp.concatenate((cp.array([x[0]]),\n",
    "                               cp.where(\n",
    "                                   cp.abs(x[1:]) > lambda_,\n",
    "                                   x[1:] - cp.sign(x[1:]) * lambda_, 0)))\n",
    "\n",
    "    def SCAD(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        To calculate SCAD penalty value;\n",
    "        #x can be a multi-dimensional tensor;\n",
    "        lambda_, a are scalars;\n",
    "        Fan and Li suggests to take a as 3.7 \n",
    "        '''\n",
    "        # here I notice the function is de facto a function of absolute value of x, therefore take absolute value first to simplify calculation\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, lambda_ * x,\n",
    "            cp.where(x < a * lambda_,\n",
    "                     (2 * a * lambda_ * x - x**2 - lambda_**2) / (2 * (a - 1)),\n",
    "                     lambda_**2 * (a + 1) / 2))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_grad(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        To calculate the gradient of SCAD wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # here decompose x to sign and its absolute value for easier calculation\n",
    "        sgn = cp.sign(x)\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, lambda_ * sgn,\n",
    "            cp.where(x < a * lambda_, (a * lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                     0))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        To calculate MCP penalty value; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # the function is a function of absolute value of x\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(x <= gamma * lambda_, lambda_ * x - x**2 / (2 * gamma),\n",
    "                        .5 * gamma * lambda_**2)\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_grad(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        To calculate MCP gradient wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        temp = cp.where(\n",
    "            cp.abs(x) < gamma * lambda_,\n",
    "            lambda_ * cp.sign(x) - x / gamma, cp.zeros_like(x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_concave(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        The value of concave part of SCAD penalty; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, 0.,\n",
    "            cp.where(x < a * lambda_,\n",
    "                     (lambda_ * x - (x**2 + lambda_**2) / 2) / (a - 1),\n",
    "                     (a + 1) / 2 * lambda_**2 - lambda_ * x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def SCAD_concave_grad(self, x, lambda_, a=3.7):\n",
    "        '''\n",
    "        The gradient of concave part of SCAD penalty wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        sgn = cp.sign(x)\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(\n",
    "            x <= lambda_, 0.,\n",
    "            cp.where(x < a * lambda_, (lambda_ * sgn - sgn * x) / (a - 1),\n",
    "                     -lambda_ * sgn))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_concave(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        The value of concave part of MCP penalty; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        # similiar as in MCP\n",
    "        x = cp.abs(x)\n",
    "        temp = cp.where(x <= gamma * lambda_, -(x**2) / (2 * gamma),\n",
    "                        (gamma * lambda_**2) / 2 - lambda_ * x)\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp\n",
    "\n",
    "    def MCP_concave_grad(self, x, lambda_, gamma):\n",
    "        '''\n",
    "        The gradient of concave part of MCP penalty wrt. input x; \n",
    "        #x can be a multi-dimensional tensor. \n",
    "        '''\n",
    "        temp = cp.where(\n",
    "            cp.abs(x) < gamma * lambda_, -x / gamma, -lambda_ * cp.sign(x))\n",
    "        temp[0] = 0.  # this is to NOT penalize intercept beta later\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Illustrations for SCAD and MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:51.955594Z",
     "start_time": "2022-08-22T20:06:51.554632Z"
    }
   },
   "outputs": [],
   "source": [
    "x = cp.arange(-15.1, 15.2, .1)\n",
    "class_temp = tensor_computation()\n",
    "\n",
    "plt.plot(x[1:-1].get(), class_temp.soft_thresholding(x, lambda_=2)[1:-1].get())\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'soft-thresholding when $\\lambda$=2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:52.800097Z",
     "start_time": "2022-08-22T20:06:51.956786Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].plot(x[1:-1].get(), cp.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].plot(x[1:-1].get(), cp.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].plot(x[1:-1].get(), cp.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].plot(x[1:-1].get(), cp.abs(2 * x)[1:-1].get(), label=\"LASSO\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='penalty')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.savefig('SCAD_MCP.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:53.559495Z",
     "start_time": "2022-08-22T20:06:52.801282Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='concave part')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.savefig('SCAD_MCP_concave.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:53.754385Z",
     "start_time": "2022-08-22T20:06:53.560706Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_concave_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_concave_grad(x, lambda_=2,\n",
    "                                           gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='concavity gradient')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:54.036063Z",
     "start_time": "2022-08-22T20:06:53.755268Z"
    }
   },
   "outputs": [],
   "source": [
    "# To plot the derivatives of SCAD and MCP\n",
    "x[np.abs(x) < 1e-10] = float(\"nan\")\n",
    "markerstyle = {\n",
    "    \"markersize\": 8,\n",
    "    \"markeredgecolor\": \"black\",\n",
    "    \"markerfacecolor\": \"w\",\n",
    "    \"linestyle\": \"none\"\n",
    "}\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=1)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 0].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[0, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=1$')\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[0, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=3.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[0, 1].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[0, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=3.7$')\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 0].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=4.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 0].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[1, 0].set_title(r'$\\lambda=2,a=3.7,\\gamma=4.7$')\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.SCAD_grad(x, lambda_=2, a=3.7)[1:-1].get(),\n",
    "               label=\"SCAD\")\n",
    "axs[1, 1].plot(x[1:-1].get(),\n",
    "               class_temp.MCP_grad(x, lambda_=2, gamma=2.7)[1:-1].get(),\n",
    "               label=\"MCP\")\n",
    "axs[1, 1].plot([0, 0], [-2, 2], marker=\".\", **markerstyle)\n",
    "axs[1, 1].set_title(r'$\\lambda=2,a=3.7,\\gamma=2.7$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=r'$\\theta$', ylabel='derivative')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 1.05),\n",
    "           ncol=1,\n",
    "           fancybox=True,\n",
    "           shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:54.079045Z",
     "start_time": "2022-08-22T20:06:54.037273Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LM_SCAD_MCP(tensor_computation):\n",
    "    '''\n",
    "    This class performs SCAD/MCP pruning on linear models.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 design_matrix,\n",
    "                 outcome,\n",
    "                 penalty,\n",
    "                 _lambda,\n",
    "                 a=3.7,\n",
    "                 gamma=3,\n",
    "                 beta_0=\"NOT DECLARED\",\n",
    "                 tol=1e-4,\n",
    "                 maxit=5000,\n",
    "                 L_convex=\"NOT DECLARED\"):\n",
    "        '''\n",
    "        Class constructor:\n",
    "        design_matrix:           the design matrix for the linear models;\n",
    "        outcome:                 the outcome for the linear model;\n",
    "        penalty:                 \"SCAD\" or \"MCP\";\n",
    "        _lambda:                 value for lambda; \n",
    "        a:                       value for a, only used when penalty set to be \"SCAD\";\n",
    "        gamma:                   value for gamma, only used when penalty set to be \"MCP\";\n",
    "        beta_0:                  initial values for beta;\n",
    "        tol:                     tolerance parameter set for beta, this is for the maximum change of beta;\n",
    "        maxit:                   maximum number of iterations allowed;\n",
    "        '''\n",
    "        assert penalty in (\n",
    "            \"SCAD\", \"MCP\"), \"Choose between \\\"SCAD\\\" or \\\"MCP\\\" for penalty\"\n",
    "        assert a > 2, \"SCAD penalty parameter, a, needs to be greater than 2\"\n",
    "        assert gamma > 0, \"MCP penalty parameter, gamma, needs to be positive\"\n",
    "        assert _lambda > 0, \"penalty paramter, lambda_, needs to be positive\"\n",
    "        assert tol > 0, \"tol should be postive\"\n",
    "        assert maxit > 0, \"maxit is the maximum iteration allowed; which needs to be positive\"\n",
    "        # Construct self\n",
    "        self.X = design_matrix\n",
    "        self.y = outcome\n",
    "        cov = (self.y - cp.mean(self.y)) @ (self.X -\n",
    "                                            cp.mean(self.X, 0).reshape(1, -1))\n",
    "        if type(beta_0) == str:\n",
    "            self.beta = cp.sign(cov)\n",
    "        else:\n",
    "            self.beta = beta_0\n",
    "        self.N = self.X.shape[0]\n",
    "        # add design matrix column for the intercept, if it's not there already\n",
    "        if cp.any(\n",
    "                cp.all(self.X == self.X[0, :], 0)\n",
    "        ):  # check if design matrix has included a column for intercept or not\n",
    "            pass\n",
    "        else:\n",
    "            intercept_design = cp.ones(self.N).reshape(self.N, 1)\n",
    "            self.X = cp.concatenate((intercept_design, self.X), 1)\n",
    "            if type(beta_0) == str:\n",
    "                self.beta = cp.concatenate((cp.array([0.]), self.beta))\n",
    "        # passing other parameters\n",
    "        self.tol = tol\n",
    "        self.maxit = maxit\n",
    "        self._lambda = _lambda\n",
    "        self.penalty = penalty\n",
    "        #        if penalty == \"SCAD\":\n",
    "        self.a = a\n",
    "        #        else:\n",
    "        self.gamma = gamma\n",
    "        self.p = self.X.shape[\n",
    "            1]  # so here p includes the intercept design matrix column\n",
    "        self.smooth_grad = cp.ones(self.p)\n",
    "        self.beta_ag = self.beta.copy()\n",
    "        self.beta_md = self.beta.copy()\n",
    "        self.k = 0\n",
    "        self.FISTA_k = 0\n",
    "        self.converged = False\n",
    "        self.obj_value = []\n",
    "        self.obj_value_ORIGINAL = []\n",
    "        self.obj_value_AG = []\n",
    "        self.obj_coord_value = []\n",
    "        self.opt_alpha = 1\n",
    "        if type(L_convex) == str:\n",
    "            self.L_convex = 1 / self.N * cp.max(\n",
    "                cp.linalg.eigh(self.X @ self.X.T)[0]).item()\n",
    "        else:\n",
    "            self.L_convex = L_convex\n",
    "        self.FISTA_beta = cp.empty_like(self.beta)\n",
    "\n",
    "    def update_smooth_grad_convex(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth convex objective component.\n",
    "        '''\n",
    "        self.smooth_grad = 1 / self.N * self.X.T @ (self.X @ self.beta_md -\n",
    "                                                    self.y)\n",
    "\n",
    "    def update_smooth_grad_SCAD(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth objective component for SCAD penalty.\n",
    "        '''\n",
    "        self.update_smooth_grad_convex()\n",
    "        self.smooth_grad += self.SCAD_concave_grad(self.beta_md,\n",
    "                                                   lambda_=self._lambda,\n",
    "                                                   a=self.a)\n",
    "\n",
    "    def update_smooth_grad_MCP(self):\n",
    "        '''\n",
    "        Update the gradient of the smooth objective component for MCP penalty.\n",
    "        '''\n",
    "        self.update_smooth_grad_convex()\n",
    "        self.smooth_grad += self.MCP_concave_grad(self.beta_md,\n",
    "                                                  lambda_=self._lambda,\n",
    "                                                  gamma=self.gamma)\n",
    "\n",
    "    def eval_obj_SCAD(self, x_temp, obj_value_name):\n",
    "        '''\n",
    "        evaluate value of the objective function.\n",
    "        '''\n",
    "        error = self.y - self.X @ x_temp\n",
    "        obj_value_name += [(error.T @ error) / (2 * self.N) +\n",
    "                           cp.sum(self.SCAD(x_temp, self._lambda, self.a))]\n",
    "\n",
    "    def eval_obj_MCP(self, x_temp, obj_value_name):\n",
    "        '''\n",
    "        evaluate value of the objective function.\n",
    "        '''\n",
    "        error = self.y - self.X @ x_temp\n",
    "        obj_value_name += [(error.T @ error) / (2 * self.N) +\n",
    "                           cp.sum(self.MCP(x_temp, self._lambda, self.gamma))]\n",
    "\n",
    "    def UAG_LM_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                    )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    #self.opt_alpha = 2/(self.k+1) #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                    )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md - self.beta_ag)) /\n",
    "                        self.opt_beta < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def UAG_restarting_LM_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        self.old_speed_norm = 0.\n",
    "        self.speed_norm = 1.\n",
    "        self.restart_k = 0\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    if self.old_speed_norm > self.speed_norm and self.k - self.restart_k >= 3:\n",
    "                        self.opt_alpha = 1.\n",
    "                        self.restart_k = self.k\n",
    "                        print(self.restart_k)\n",
    "                    else:\n",
    "                        pass\n",
    "                        #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        self.opt_alpha = 2 / (\n",
    "                            1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                        )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md_old = self.beta_md.copy()\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.old_speed_norm = self.speed_norm\n",
    "                    self.speed_norm = cp.linalg.norm(self.beta_md -\n",
    "                                                     self.beta_md_old)\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .99 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    if self.old_speed_norm > self.speed_norm and self.k - self.restart_k > 3:\n",
    "                        self.opt_alpha = 1.\n",
    "                        self.restart_k = self.k\n",
    "                        print(self.restart_k)\n",
    "                    else:\n",
    "                        pass\n",
    "                        #self.opt_alpha = 2/(self.k+1)**0.3 #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        #self.opt_lambda = self.k/2*self.opt_beta #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                        self.opt_alpha = 2 / (\n",
    "                            1 + cp.sqrt(1 + 4 / self.opt_alpha**2)\n",
    "                        )  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.opt_lambda = self.opt_beta / self.opt_alpha  #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md_old = self.beta_md.copy()\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.old_speed_norm = self.speed_norm\n",
    "                    self.speed_norm = cp.linalg.norm(self.beta_md -\n",
    "                                                     self.beta_md_old)\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def FISTA_LM_SCAD_MCP(self):\n",
    "        '''\n",
    "        Carry out FISTA procedure to find out l1 penalized minimizer.\n",
    "        '''\n",
    "        FISTA_t_new = 1.\n",
    "        FISTA_converged = False\n",
    "        x_new, x_old = cp.empty_like(self.beta_md), cp.empty_like(\n",
    "            self.beta_md\n",
    "        )  # beta_md here is y; it plays a very different role here!\n",
    "        self.update_smooth_grad_convex()\n",
    "        x_new = self.soft_thresholding(\n",
    "            self.beta_md - self.smooth_grad / self.L_convex,\n",
    "            self._lambda / self.L_convex)\n",
    "        if self.penalty == \"SCAD\":\n",
    "            self.eval_obj_SCAD(x_new, self.obj_value)\n",
    "        else:\n",
    "            self.eval_obj_MCP(x_new, self.obj_value)\n",
    "        while (not FISTA_converged) and self.FISTA_k <= self.maxit:\n",
    "            self.FISTA_k += 1\n",
    "            self.update_smooth_grad_convex()\n",
    "            x_old = x_new.copy()\n",
    "            x_new = self.soft_thresholding(\n",
    "                self.beta_md - self.smooth_grad / self.L_convex,\n",
    "                self._lambda / self.L_convex)\n",
    "            FISTA_t_old = FISTA_t_new\n",
    "            FISTA_t_new = (1 + cp.sqrt(1 + 4 * FISTA_t_new**2)) / 2\n",
    "            diff_temp = x_new - x_old\n",
    "            self.beta_md = x_new + (FISTA_t_old - 1) / FISTA_t_new * diff_temp\n",
    "            FISTA_converged = cp.all(\n",
    "                cp.max(cp.abs(diff_temp)) < self.tol\n",
    "            ).item(\n",
    "            ) and self.FISTA_k != 1  # since when FISTA_k=1, x_new and x_old are the same\n",
    "            if self.penalty == \"SCAD\":\n",
    "                self.eval_obj_SCAD(x_new, self.obj_value)\n",
    "            else:\n",
    "                self.eval_obj_MCP(x_new, self.obj_value)\n",
    "        self.FISTA_beta = x_new  # because we used self.beta_md as y all the time, now it should be fixed\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "\n",
    "    def Two_step_FISTA_UAG(self):\n",
    "        '''\n",
    "        Carry out the two step combining FISTA and Ghadimi's AG.\n",
    "        '''\n",
    "        self.FISTA_LM_SCAD_MCP()\n",
    "        self.k = self.FISTA_k  # so FISTA iterations will also count into the number of iterations\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "        return self.UAG_LM_SCAD_MCP()\n",
    "\n",
    "    def Two_step_FISTA_ISTA(self):\n",
    "        '''\n",
    "        Carry out the two step combining FISTA and ISTA.\n",
    "        '''\n",
    "        self.FISTA_LM_SCAD_MCP()\n",
    "        self.k = self.FISTA_k  # so FISTA iterations will also count into the number of iterations\n",
    "        self.beta = self.FISTA_beta.copy()\n",
    "        self.beta_ag = self.FISTA_beta.copy()\n",
    "        self.beta_md = self.FISTA_beta.copy()\n",
    "        return self.vanilla_proximal()\n",
    "\n",
    "    def UAG_LM_SCAD_MCP_Ghadimi_parameter(self):\n",
    "        '''\n",
    "        Carry out the optimization.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.opt_beta = .5 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        self.k + 1\n",
    "                    )  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_lambda = self.k / 2 * self.opt_beta  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_alpha = 2/(1+cp.sqrt(1+4/self.opt_alpha**2)) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    #self.opt_lambda = self.opt_beta/self.opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(\n",
    "                            cp.abs(self.beta_md - self.beta_ag) /\n",
    "                            self.opt_beta) < self.tol).item()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.opt_beta = .5 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.opt_alpha = 2 / (\n",
    "                        self.k + 1\n",
    "                    )  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    self.opt_lambda = self.k / 2 * self.opt_beta  #parameter setting based on Ghadimi and Lan's exemplified Lemma\n",
    "                    #self.opt_alpha = 2/(1+cp.sqrt(1+4/self.opt_alpha**2)) #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper bound\n",
    "                    #self.opt_lambda = self.opt_beta/self.opt_alpha #parameter settings based on minimizing Ghadimi and Lan's rate of convergence error upper\n",
    "                    self.beta_md = (\n",
    "                        1 - self.opt_alpha\n",
    "                    ) * self.beta_ag + self.opt_alpha * self.beta\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta = self.soft_thresholding(\n",
    "                        self.beta - self.opt_lambda * self.smooth_grad,\n",
    "                        self.opt_lambda * self._lambda)\n",
    "                    self.beta_ag = self.soft_thresholding(\n",
    "                        self.beta_md - self.opt_beta * self.smooth_grad,\n",
    "                        self.opt_beta * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md - self.beta_ag)) /\n",
    "                        self.opt_beta < self.tol).item()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def vanilla_proximal(self):\n",
    "        '''\n",
    "        Carry out optimization using vanilla gradient descent.\n",
    "        '''\n",
    "        if self.penalty == \"SCAD\":\n",
    "            L = max([self.L_convex, 1 / (self.a - 1)])\n",
    "            self.vanilla_stepsize = 1 / L\n",
    "            self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "            self.old_beta = self.beta_md - 10.\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.update_smooth_grad_SCAD()\n",
    "                    self.beta_md = self.soft_thresholding(\n",
    "                        self.beta_md -\n",
    "                        self.vanilla_stepsize * self.smooth_grad,\n",
    "                        self.vanilla_stepsize * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md -\n",
    "                                      self.old_beta)) < self.tol).item()\n",
    "                    self.old_beta = self.beta_md.copy()\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            L = max([self.L_convex, 1 / self.gamma])\n",
    "            self.vanilla_stepsize = 1 / L\n",
    "            self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "            self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "            self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "            self.old_beta = self.beta_md - 10.\n",
    "            while not self.converged:\n",
    "                self.k += 1\n",
    "                if self.k <= self.maxit:\n",
    "                    self.update_smooth_grad_MCP()\n",
    "                    self.beta_md = self.soft_thresholding(\n",
    "                        self.beta_md -\n",
    "                        self.vanilla_stepsize * self.smooth_grad,\n",
    "                        self.vanilla_stepsize * self._lambda)\n",
    "                    self.converged = cp.all(\n",
    "                        cp.max(cp.abs(self.beta_md -\n",
    "                                      self.old_beta)) < self.tol).item()\n",
    "                    self.old_beta = self.beta_md.copy()\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    break\n",
    "        return self.report_results()\n",
    "\n",
    "    def coordinate_descent(self):\n",
    "        res = self.y - self.X @ self.beta_md\n",
    "        while not self.converged:\n",
    "            self.k += 1\n",
    "            if self.k <= self.maxit:\n",
    "                self.beta_md_old = self.beta_md.copy()\n",
    "                if self.penalty == \"SCAD\":\n",
    "                    for j in range(self.p):\n",
    "                        res += self.X[:, j] * self.beta_md[j]\n",
    "                        self.beta_md[j] = 1 / self.N * self.X[:, j].T @ res\n",
    "                        res -= self.X[:, j] * self.beta_md[j]\n",
    "                        if j != 0:\n",
    "                            _sign = cp.sign(self.beta_md[j])\n",
    "                            self.beta_md[j] = cp.abs(self.beta_md[j])\n",
    "                            self.beta_md[j] = cp.where(\n",
    "                                self.beta_md[j] <= self._lambda, 0.,\n",
    "                                cp.where(\n",
    "                                    self.beta_md[j] <= 2 * self._lambda,\n",
    "                                    _sign * (self.beta_md[j] - self._lambda),\n",
    "                                    cp.where(\n",
    "                                        self.beta_md[j] <=\n",
    "                                        self.a * self._lambda,\n",
    "                                        _sign *\n",
    "                                        ((self.a - 1.) * self.beta_md[j] -\n",
    "                                         self._lambda * self.a) /\n",
    "                                        (self.a - 2.),\n",
    "                                        _sign * self.beta_md[j])))\n",
    "                    self.eval_obj_SCAD(self.beta_md, self.obj_coord_value)\n",
    "                    self.eval_obj_SCAD(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_SCAD(self.beta_ag, self.obj_value_AG)\n",
    "                else:\n",
    "                    for j in range(self.p):\n",
    "                        res += self.X[:, j] * self.beta_md[j]\n",
    "                        self.beta_md[j] = 1 / self.N * self.X[:, j].T @ res\n",
    "                        res -= self.X[:, j] * self.beta_md[j]\n",
    "                        if j != 0:\n",
    "                            _sign = cp.sign(self.beta_md[j])\n",
    "                            self.beta_md[j] = cp.abs(self.beta_md[j])\n",
    "                            self.beta_md[j] = cp.where(\n",
    "                                self.beta_md[j] <= self._lambda, 0.,\n",
    "                                cp.where(\n",
    "                                    self.beta_md[j] <=\n",
    "                                    self._lambda * self.gamma,\n",
    "                                    _sign * self.gamma *\n",
    "                                    (self.beta_md[j] - self._lambda) /\n",
    "                                    (self.gamma - 1.),\n",
    "                                    _sign * self.beta_md[j]))\n",
    "                    self.eval_obj_MCP(self.beta_md, self.obj_coord_value)\n",
    "                    self.eval_obj_MCP(self.beta, self.obj_value_ORIGINAL)\n",
    "                    self.eval_obj_MCP(self.beta_ag, self.obj_value_AG)\n",
    "                self.converged = cp.all(\n",
    "                    cp.max(cp.abs(self.beta_md - self.beta_md_old)) < self.tol)\n",
    "            else:\n",
    "                break\n",
    "        return self.report_results()\n",
    "\n",
    "    def report_results(self):\n",
    "        '''\n",
    "        A results reporting tool.\n",
    "        '''\n",
    "        #self.beta_md[cp.abs(self.beta_md)<self.tol] = 0 # for those estimates below tolerance parameter, set them to 0\n",
    "        self.estimates_constructor = collections.namedtuple(\n",
    "            'Estimates', [\n",
    "                'beta_est', 'converged', 'num_of_iterations', 'obj_values',\n",
    "                'obj_values_orignal', 'obj_values_AG', 'obj_coord_values',\n",
    "                'FISTA_estimates'\n",
    "            ])\n",
    "        results = self.estimates_constructor(self.beta_md, self.converged,\n",
    "                                             self.k, self.obj_value,\n",
    "                                             self.obj_value_ORIGINAL,\n",
    "                                             self.obj_value_AG,\n",
    "                                             self.obj_coord_value,\n",
    "                                             self.FISTA_beta)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong rule implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:06:54.092243Z",
     "start_time": "2022-08-22T20:06:54.080878Z"
    }
   },
   "outputs": [],
   "source": [
    "def lambda_max_LM(X, y):\n",
    "    # suppose that X, y is already standardized to make it faster, but X here is supposed not to have intercept column\n",
    "    #     X_temp = X.copy()\n",
    "    #     X_temp = X_temp[:,1:]\n",
    "    #     X_temp -= cp.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= cp.std(X_temp,0)\n",
    "    y_temp = y.copy()\n",
    "    y_temp -= cp.mean(y)\n",
    "    y_temp /= cp.std(y)\n",
    "    grad_at_0 = cp.abs(y_temp @ X / len(y_temp))\n",
    "    lambda_max = cp.max(grad_at_0)\n",
    "    return lambda_max\n",
    "\n",
    "\n",
    "def strong_rule_seq_LM(X, y, beta_old, lambda_new, lambda_old):\n",
    "    # suppose that X, y is already standardized to make it faster, but X here is supposed not to have intercept column\n",
    "    X_temp = X.copy()\n",
    "    #     X_temp -= cp.mean(X_temp,0).reshape(1,-1)\n",
    "    #     X_temp /= cp.std(X_temp,0)\n",
    "    #     y_temp = y.copy()\n",
    "    #     y_temp -= cp.mean(y)\n",
    "    #     y_temp /= cp.std(y)\n",
    "    grad = cp.abs((y - X_temp @ beta_old[1:]) @ X_temp / (2 * len(y)))\n",
    "    eliminated = (grad < 2 * lambda_new - lambda_old\n",
    "                  )  # True means the value gets eliminated\n",
    "    #    eliminated = cp.concatenate(([False], eliminated)) # because intercept coefficient is not penalized\n",
    "    return eliminated\n",
    "\n",
    "\n",
    "def fit_LM(X, y, lambda_seq, penalty, a=3.7, gamma=3., tol=1e-5):\n",
    "    '''\n",
    "    A function to fit SCAD/MCP penalized LM with given lambda_seq (in a decreasing order), under strong rules; with X being standardized automatically (no intercept column); lambda_max will be calculated and added at the begining of lambda sequence.\n",
    "    '''\n",
    "    X_temp = X.copy()\n",
    "    y_temp = y.copy()\n",
    "    lambda_seq_temp = lambda_seq.copy()\n",
    "    X_temp -= cp.mean(X, 0).reshape(1, -1)\n",
    "    y_temp -= cp.mean(y)\n",
    "    X_temp /= cp.std(X, 0)\n",
    "    y_temp /= cp.std(y)\n",
    "    beta_est = cp.zeros((len(lambda_seq_temp) + 1, X.shape[1] + 1))\n",
    "    lambda_seq_temp = cp.concatenate(\n",
    "        (cp.array([lambda_max_LM(X=X_temp, y=y_temp)]), lambda_seq_temp))\n",
    "    elim = cp.array([True] * X.shape[1])\n",
    "    for i in cp.arange(len(lambda_seq_temp) - 1):\n",
    "        elim_temp = strong_rule_seq_LM(X_temp,\n",
    "                                       y_temp,\n",
    "                                       beta_old=beta_est[i, :],\n",
    "                                       lambda_new=lambda_seq_temp[i + 1],\n",
    "                                       lambda_old=lambda_seq_temp[i])\n",
    "        elim = cp.logical_and(elim, elim_temp) if i > 0 else cp.array(\n",
    "            [True] * X.shape[1]\n",
    "        )  # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        #         elim = elim_temp if i>0 else cp.array([True]*X.shape[1]) # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        temp_beta = beta_est[i, :]\n",
    "        cls = LM_SCAD_MCP(\n",
    "            design_matrix=X_temp[:, cp.invert(elim)],\n",
    "            outcome=y_temp,\n",
    "            penalty=penalty,\n",
    "            _lambda=lambda_seq_temp[i + 1],  # .6 works\n",
    "            a=a,\n",
    "            gamma=gamma,\n",
    "            beta_0=temp_beta[cp.concatenate(\n",
    "                (cp.array([True]), cp.invert(elim)))],\n",
    "            tol=tol,\n",
    "            maxit=1000)\n",
    "        beta_temp = cp.zeros(X.shape[1] + 1)\n",
    "        beta_temp[cp.concatenate(\n",
    "            (cp.array([True]), cp.invert(elim)))] = cls.UAG_LM_SCAD_MCP()[0]\n",
    "        beta_est[i + 1, :] = beta_temp\n",
    "    beta_est[:, 1:] *= cp.std(y) / (cp.std(X, 0).reshape(1, -1))\n",
    "    beta_est[:, 0] = cp.mean(\n",
    "        y\n",
    "    )  # here we just assume that X is standardized, if not, more coding details are required\n",
    "    return beta_est\n",
    "\n",
    "\n",
    "def fit_LM_coord(X, y, lambda_seq, penalty, a=3.7, gamma=3., tol=1e-5):\n",
    "    '''\n",
    "    A function to fit SCAD/MCP penalized LM with given lambda_seq (in a decreasing order), under strong rules; with X being standardized automatically (no intercept column); lambda_max will be calculated and added at the begining of lambda sequence.\n",
    "    '''\n",
    "    X_temp = X.copy()\n",
    "    y_temp = y.copy()\n",
    "    lambda_seq_temp = lambda_seq.copy()\n",
    "    X_temp -= cp.mean(X, 0).reshape(1, -1)\n",
    "    y_temp -= cp.mean(y)\n",
    "    X_temp /= cp.std(X, 0)\n",
    "    y_temp /= cp.std(y)\n",
    "    beta_est = cp.zeros((len(lambda_seq_temp) + 1, X.shape[1] + 1))\n",
    "    lambda_seq_temp = cp.concatenate(\n",
    "        (cp.array([lambda_max_LM(X=X_temp, y=y_temp)]), lambda_seq_temp))\n",
    "    elim = cp.array([True] * X.shape[1])\n",
    "    for i in cp.arange(len(lambda_seq_temp) - 1):\n",
    "        elim_temp = strong_rule_seq_LM(X_temp,\n",
    "                                       y_temp,\n",
    "                                       beta_old=beta_est[i, :],\n",
    "                                       lambda_new=lambda_seq_temp[i + 1],\n",
    "                                       lambda_old=lambda_seq_temp[i])\n",
    "        elim = cp.logical_and(elim, elim_temp) if i > 0 else cp.array(\n",
    "            [True] * X.shape[1]\n",
    "        )  # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        #         elim = elim_temp if i>0 else cp.array([True]*X.shape[1]) # because at lambda_max all penalized coefficinets should be eliminated, then when some coefficinets start not to be eliminated, it keeps in the estimates\n",
    "        temp_beta = beta_est[i, :]\n",
    "        cls = LM_SCAD_MCP(\n",
    "            design_matrix=X_temp[:, cp.invert(elim)],\n",
    "            outcome=y_temp,\n",
    "            penalty=penalty,\n",
    "            _lambda=lambda_seq_temp[i + 1],  # .6 works\n",
    "            a=a,\n",
    "            gamma=gamma,\n",
    "            beta_0=temp_beta[cp.concatenate(\n",
    "                (cp.array([True]), cp.invert(elim)))],\n",
    "            tol=tol,\n",
    "            maxit=1000,\n",
    "            L_convex=1.\n",
    "        )  # for coordinate descent, L Lipschitz constant is not needed to compute\n",
    "        beta_temp = cp.zeros(X.shape[1] + 1)\n",
    "        beta_temp[cp.concatenate(\n",
    "            (cp.array([True]), cp.invert(elim)))] = cls.coordinate_descent()[0]\n",
    "        beta_est[i + 1, :] = beta_temp\n",
    "    beta_est[:, 1:] *= cp.std(y) / (cp.std(X, 0).reshape(1, -1))\n",
    "    beta_est[:, 0] = cp.mean(\n",
    "        y\n",
    "    )  # here we just assume that X is standardized, if not, more coding details are required\n",
    "    return beta_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some simulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T20:16:08.500875Z",
     "start_time": "2021-08-19T20:16:08.490153Z"
    }
   },
   "source": [
    "## performance of signal recovery ($2-$norm, $\\infty-$norm, PPV/NPV) based on best testing set performance, with SNR=$1,3,7,10$, Toeplitz being $0.1,0.5,0.9$, and N=$1000$, p=$2050$ with each $10$ coefficients simulated from $N(.5,1),N(5,2),N(10,3),N(20,4),N(50,5)$, sparsely located in the array, with $500$ zeros in-between \n",
    "\n",
    "### SCAD `results_SCAD_signal_recovery.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.830114Z",
     "start_time": "2022-08-22T20:06:54.093396Z"
    }
   },
   "outputs": [],
   "source": [
    "data = robjects.r(\"\"\"\n",
    "library(MASS)\n",
    "x = array(0, dim = c(4, 3, 100, 2000, 2052)) # first column will be y\n",
    "true_betas = array(0, dim = c(4,3,100,2051))\n",
    "for (i in 1:4){\n",
    "for (j in 1:3){\n",
    "for (seed in 1:100){\n",
    "set.seed(seed)\n",
    "true_beta = c(rnorm(10, .5, 1), rep(0,500), rnorm(10, 5, 2), rep(0,500), rnorm(10, 10, 3), rep(0,500), rnorm(10, 20, 4), rep(0,500), rnorm(10, 50, 5))\n",
    "true_betas[i,j,seed,-1] = true_beta\n",
    "X_cov = toeplitz((c(0.1,0.5,0.9)[j])^c(0:(length(true_beta)-1)))\n",
    "X_temp = mvrnorm(2000, rep(0,2050), X_cov)\n",
    "X_temp = scale(X_temp)\n",
    "x[i,j,seed,,-c(1,2)] = X_temp\n",
    "true_sigma_sim = sqrt(t(true_beta)%*%X_cov%*%true_beta/(c(1,3,7,10)[i]))\n",
    "y_temp = X_temp%*%true_beta + rnorm(2000,0,true_sigma_sim) # here I simply let the true intercept coefficient to be 0\n",
    "x[i,j,seed,,2] = 1\n",
    "x[i,j,seed,,1] = y_temp\n",
    "}\n",
    "}\n",
    "}\n",
    "\"\"\")\n",
    "print(\"simulating from R done!\")\n",
    "SCAD_sim = np.array(robjects.r[\"x\"])\n",
    "SCAD_true_beta = np.array(robjects.r[\"true_betas\"])\n",
    "results_SCAD_signal_recovery = cp.zeros((4, 3, 100, 5))\n",
    "\n",
    "for i, j, seed in itertools.product(range(4), range(3), range(100)):\n",
    "    # import to GPU only when it's needed\n",
    "    X_sim = cp.array(SCAD_sim[i, j, seed, :1000, 1:])\n",
    "    y_sim = cp.array(SCAD_sim[i, j, seed, :1000, 0])\n",
    "    lambda_seq = cp.linspace(lambda_max_LM(X_sim[:, 1:], y_sim),\n",
    "                             0,\n",
    "                             num=50,\n",
    "                             endpoint=False)\n",
    "    temp_beta = fit_LM(X_sim[:, 1:],\n",
    "                       y_sim,\n",
    "                       lambda_seq=lambda_seq,\n",
    "                       penalty=\"SCAD\",\n",
    "                       a=3.7,\n",
    "                       gamma=3.)\n",
    "    # testing which beta performs the best\n",
    "    testing_X = cp.array(SCAD_sim[i, j, seed, 1000:, 1:])\n",
    "    testing_y = cp.array(SCAD_sim[i, j, seed, 1000:, 0])\n",
    "    testing_temp = testing_X @ temp_beta.T - testing_y.reshape(\n",
    "        -1, 1)  # each column is error for each beta\n",
    "    testing_error = cp.linalg.norm(testing_temp, axis=0)\n",
    "    # choose the right beta\n",
    "    beta_ind = cp.argmin(testing_error)\n",
    "    chosen_beta = temp_beta[beta_ind, :]\n",
    "    temp_true_beta = cp.array(SCAD_true_beta[i, j, seed, :])\n",
    "    norm2_error = (cp.linalg.norm(chosen_beta - temp_true_beta, 2) /\n",
    "                   cp.linalg.norm(temp_true_beta, 2))**2\n",
    "    norminfity_error = cp.linalg.norm(chosen_beta - temp_true_beta, cp.inf)\n",
    "    NPV = cp.sum(\n",
    "        cp.logical_and(chosen_beta == 0., temp_true_beta == 0.) * 1.) / cp.sum(\n",
    "            (chosen_beta == 0.) * 1.)\n",
    "    PPV = cp.sum(\n",
    "        cp.logical_and(chosen_beta != 0., temp_true_beta != 0.) * 1.) / cp.sum(\n",
    "            (chosen_beta != 0.) * 1.)\n",
    "    active_set_cardi = cp.sum((chosen_beta != 0.) * 1.)\n",
    "    results_SCAD_signal_recovery[i, j, seed, 0] = norm2_error\n",
    "    results_SCAD_signal_recovery[i, j, seed, 1] = norminfity_error\n",
    "    results_SCAD_signal_recovery[i, j, seed, 2] = PPV\n",
    "    results_SCAD_signal_recovery[i, j, seed, 3] = NPV\n",
    "    results_SCAD_signal_recovery[i, j, seed, 4] = active_set_cardi\n",
    "\n",
    "cp.save(\"results_SCAD_signal_recovery\", results_SCAD_signal_recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.831153Z",
     "start_time": "2022-08-22T20:07:32.831145Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_SCAD_signal_recovery_mean = cp.mean(results_SCAD_signal_recovery, 2)\n",
    "# print(\"Sample mean:\")\n",
    "# print(\"2-norm: \",results_SCAD_signal_recovery_mean[...,0])\n",
    "# print(\"infinity-norm: \",results_SCAD_signal_recovery_mean[...,1])\n",
    "# print(\"PPV: \",results_SCAD_signal_recovery_mean[...,2])\n",
    "# print(\"NPV: \",results_SCAD_signal_recovery_mean[...,3])\n",
    "# results_SCAD_signal_recovery_se = cp.std(results_SCAD_signal_recovery, 2)\n",
    "# print(\"Sample se:\")\n",
    "# print(\"2-norm: \",results_SCAD_signal_recovery_se[...,0])\n",
    "# print(\"infinity-norm: \",results_SCAD_signal_recovery_se[...,1])\n",
    "# print(\"PPV: \",results_SCAD_signal_recovery_se[...,2])\n",
    "# print(\"NPV: \",results_SCAD_signal_recovery_se[...,3])\n",
    "\n",
    "# results_SCAD_signal_recovery_median = cp.median(results_SCAD_signal_recovery, 2)\n",
    "# print(\"Sample median:\")\n",
    "# print(\"2-norm: \",results_SCAD_signal_recovery_median[...,0])\n",
    "# print(\"infinity-norm: \",results_SCAD_signal_recovery_median[...,1])\n",
    "# print(\"PPV: \",results_SCAD_signal_recovery_median[...,2])\n",
    "# print(\"NPV: \",results_SCAD_signal_recovery_median[...,3])\n",
    "# results_SCAD_signal_recovery_mad = mad(results_SCAD_signal_recovery.get(), 2)\n",
    "# print(\"Sample MAD:\")\n",
    "# print(\"2-norm: \",results_SCAD_signal_recovery_mad[...,0])\n",
    "# print(\"infinity-norm: \",results_SCAD_signal_recovery_mad[...,1])\n",
    "# print(\"PPV: \",results_SCAD_signal_recovery_mad[...,2])\n",
    "# print(\"NPV: \",results_SCAD_signal_recovery_mad[...,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP `results_MCP_signal_recovery.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.832320Z",
     "start_time": "2022-08-22T20:07:32.832312Z"
    }
   },
   "outputs": [],
   "source": [
    "# data generation from R end is the same as above, so it's not a good idea to do it again.\n",
    "# data = robjects.r(\"\"\"\n",
    "# library(MASS)\n",
    "# x = array(0, dim = c(4, 3, 100, 2000, 2052)) # first column will be y\n",
    "# true_betas = array(0, dim = c(4,3,100,2051))\n",
    "# for (i in 1:4){\n",
    "# for (j in 1:3){\n",
    "# for (seed in 1:100){\n",
    "# set.seed(seed)\n",
    "# true_beta = c(rnorm(10, .5, 1), rep(0,500), rnorm(10, 5, 2), rep(0,500), rnorm(10, 10, 3), rep(0,500), rnorm(10, 20, 4), rep(0,500), rnorm(10, 50, 5))\n",
    "# true_betas[i,j,seed,-1] = true_beta\n",
    "# X_cov = toeplitz((c(0.1,0.5,0.9)[j])^c(0:(length(true_beta)-1)))\n",
    "# X_temp = mvrnorm(2000, rep(0,2050), X_cov)\n",
    "# X_temp = scale(X_temp)\n",
    "# x[i,j,seed,,-c(1,2)] = X_temp\n",
    "# true_sigma_sim = sqrt(t(true_beta)%*%X_cov%*%true_beta/(c(1,3,7,10)[i]))\n",
    "# y_temp = X_temp%*%true_beta + rnorm(2000,0,true_sigma_sim) # here I simply let the true intercept coefficient to be 0\n",
    "# x[i,j,seed,,2] = 1\n",
    "# x[i,j,seed,,1] = y_temp\n",
    "# }\n",
    "# }\n",
    "# }\n",
    "# \"\"\")\n",
    "# print(\"simulating from R done!\")\n",
    "MCP_sim = SCAD_sim.copy()  #np.array(robjects.r[\"x\"])\n",
    "MCP_true_beta = SCAD_true_beta.copy()  #np.array(robjects.r[\"true_betas\"])\n",
    "results_MCP_signal_recovery = cp.zeros((4, 3, 100, 5))\n",
    "\n",
    "for i, j, seed in itertools.product(range(4), range(3), range(100)):\n",
    "    # import to GPU only when it's needed\n",
    "    X_sim = cp.array(MCP_sim[i, j, seed, :1000, 1:])\n",
    "    y_sim = cp.array(MCP_sim[i, j, seed, :1000, 0])\n",
    "    lambda_seq = cp.linspace(lambda_max_LM(X_sim[:, 1:], y_sim),\n",
    "                             0,\n",
    "                             num=50,\n",
    "                             endpoint=False)\n",
    "    temp_beta = fit_LM(X_sim[:, 1:],\n",
    "                       y_sim,\n",
    "                       lambda_seq=lambda_seq,\n",
    "                       penalty=\"MCP\",\n",
    "                       a=3.7,\n",
    "                       gamma=3.)\n",
    "    # testing which beta performs the best\n",
    "    testing_X = cp.array(MCP_sim[i, j, seed, 1000:, 1:])\n",
    "    testing_y = cp.array(MCP_sim[i, j, seed, 1000:, 0])\n",
    "    testing_temp = testing_X @ temp_beta.T - testing_y.reshape(\n",
    "        -1, 1)  # each column is error for each beta\n",
    "    testing_error = cp.linalg.norm(testing_temp, axis=0)\n",
    "    # choose the right beta\n",
    "    beta_ind = cp.argmin(testing_error)\n",
    "    chosen_beta = temp_beta[beta_ind, :]\n",
    "    temp_true_beta = cp.array(MCP_true_beta[i, j, seed, :])\n",
    "    norm2_error = (cp.linalg.norm(chosen_beta - temp_true_beta, 2) /\n",
    "                   cp.linalg.norm(temp_true_beta, 2))**2\n",
    "    norminfity_error = cp.linalg.norm(chosen_beta - temp_true_beta, cp.inf)\n",
    "    NPV = cp.sum(\n",
    "        cp.logical_and(chosen_beta == 0., temp_true_beta == 0.) * 1.) / cp.sum(\n",
    "            (chosen_beta == 0.) * 1.)\n",
    "    PPV = cp.sum(\n",
    "        cp.logical_and(chosen_beta != 0., temp_true_beta != 0.) * 1.) / cp.sum(\n",
    "            (chosen_beta != 0.) * 1.)\n",
    "    active_set_cardi = cp.sum((chosen_beta != 0.) * 1.)\n",
    "    results_MCP_signal_recovery[i, j, seed, 0] = norm2_error\n",
    "    results_MCP_signal_recovery[i, j, seed, 1] = norminfity_error\n",
    "    results_MCP_signal_recovery[i, j, seed, 2] = PPV\n",
    "    results_MCP_signal_recovery[i, j, seed, 3] = NPV\n",
    "    results_MCP_signal_recovery[i, j, seed, 4] = active_set_cardi\n",
    "\n",
    "cp.save(\"results_MCP_signal_recovery\", results_MCP_signal_recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.833230Z",
     "start_time": "2022-08-22T20:07:32.833222Z"
    }
   },
   "outputs": [],
   "source": [
    "# results_MCP_signal_recovery_mean = cp.mean(results_MCP_signal_recovery, 2)\n",
    "# print(\"Sample mean:\")\n",
    "# print(\"2-norm: \",results_MCP_signal_recovery_mean[...,0])\n",
    "# print(\"infinity-norm: \",results_MCP_signal_recovery_mean[...,1])\n",
    "# print(\"PPV: \",results_MCP_signal_recovery_mean[...,2])\n",
    "# print(\"NPV: \",results_MCP_signal_recovery_mean[...,3])\n",
    "# results_MCP_signal_recovery_se = cp.std(results_MCP_signal_recovery, 2)\n",
    "# print(\"Sample se:\")\n",
    "# print(\"2-norm: \",results_MCP_signal_recovery_se[...,0])\n",
    "# print(\"infinity-norm: \",results_MCP_signal_recovery_se[...,1])\n",
    "# print(\"PPV: \",results_MCP_signal_recovery_se[...,2])\n",
    "# print(\"NPV: \",results_MCP_signal_recovery_se[...,3])\n",
    "\n",
    "# results_MCP_signal_recovery_median = cp.median(results_MCP_signal_recovery, 2)\n",
    "# print(\"Sample median:\")\n",
    "# print(\"2-norm: \",results_MCP_signal_recovery_median[...,0])\n",
    "# print(\"infinity-norm: \",results_MCP_signal_recovery_median[...,1])\n",
    "# print(\"PPV: \",results_MCP_signal_recovery_median[...,2])\n",
    "# print(\"NPV: \",results_MCP_signal_recovery_median[...,3])\n",
    "# results_MCP_signal_recovery_mad = mad(results_MCP_signal_recovery.get(), 2)\n",
    "# print(\"Sample MAD:\")\n",
    "# print(\"2-norm: \",results_MCP_signal_recovery_mad[...,0])\n",
    "# print(\"infinity-norm: \",results_MCP_signal_recovery_mad[...,1])\n",
    "# print(\"PPV: \",results_MCP_signal_recovery_mad[...,2])\n",
    "# print(\"NPV: \",results_MCP_signal_recovery_mad[...,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R ncvreg simulations\n",
    "## SCAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.834083Z",
     "start_time": "2022-08-22T20:07:32.834075Z"
    }
   },
   "outputs": [],
   "source": [
    "# R_SCAD_signal=np.load(\"R_results_SCAD_signal_recovery.npy\")\n",
    "# R_SCAD_signal=np.reshape(R_SCAD_signal, (4,3,100,4),\"F\") # keep array dimension true\n",
    "# R_SCAD_signal = cp.array(R_SCAD_signal)\n",
    "\n",
    "# R_SCAD_signal_mean = cp.mean(R_SCAD_signal, 2)\n",
    "# print(\"Sample mean:\")\n",
    "# print(\"2-norm: \",R_SCAD_signal_mean[...,0])\n",
    "# print(\"infinity-norm: \",R_SCAD_signal_mean[...,1])\n",
    "# print(\"PPV: \",R_SCAD_signal_mean[...,2])\n",
    "# print(\"NPV: \",R_SCAD_signal_mean[...,3])\n",
    "# R_SCAD_signal_se = cp.std(R_SCAD_signal, 2)\n",
    "# print(\"Sample se:\")\n",
    "# print(\"2-norm: \",R_SCAD_signal_se[...,0])\n",
    "# print(\"infinity-norm: \",R_SCAD_signal_se[...,1])\n",
    "# print(\"PPV: \",R_SCAD_signal_se[...,2])\n",
    "# print(\"NPV: \",R_SCAD_signal_se[...,3])\n",
    "\n",
    "# R_SCAD_signal_median = cp.median(R_SCAD_signal, 2)\n",
    "# print(\"Sample median:\")\n",
    "# print(\"2-norm: \",R_SCAD_signal_median[...,0])\n",
    "# print(\"infinity-norm: \",R_SCAD_signal_median[...,1])\n",
    "# print(\"PPV: \",R_SCAD_signal_median[...,2])\n",
    "# print(\"NPV: \",R_SCAD_signal_median[...,3])\n",
    "# R_SCAD_signal_mad = mad(R_SCAD_signal.get(), 2)\n",
    "# print(\"Sample MAD:\")\n",
    "# print(\"2-norm: \",R_SCAD_signal_mad[...,0])\n",
    "# print(\"infinity-norm: \",R_SCAD_signal_mad[...,1])\n",
    "# print(\"PPV: \",R_SCAD_signal_mad[...,2])\n",
    "# print(\"NPV: \",R_SCAD_signal_mad[...,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T20:07:32.834861Z",
     "start_time": "2022-08-22T20:07:32.834854Z"
    }
   },
   "outputs": [],
   "source": [
    "# R_MCP_signal=np.load(\"R_results_MCP_signal_recovery.npy\")\n",
    "# R_MCP_signal=np.reshape(R_MCP_signal, (4,3,100,4),\"F\") # keep array dimension true\n",
    "# R_MCP_signal = cp.array(R_MCP_signal)\n",
    "\n",
    "# R_MCP_signal_mean = cp.mean(R_MCP_signal, 2)\n",
    "# print(\"Sample mean:\")\n",
    "# print(\"2-norm: \",R_MCP_signal_mean[...,0])\n",
    "# print(\"infinity-norm: \",R_MCP_signal_mean[...,1])\n",
    "# print(\"PPV: \",R_MCP_signal_mean[...,2])\n",
    "# print(\"NPV: \",R_MCP_signal_mean[...,3])\n",
    "# R_MCP_signal_se = cp.std(R_MCP_signal, 2)\n",
    "# print(\"Sample se:\")\n",
    "# print(\"2-norm: \",R_MCP_signal_se[...,0])\n",
    "# print(\"infinity-norm: \",R_MCP_signal_se[...,1])\n",
    "# print(\"PPV: \",R_MCP_signal_se[...,2])\n",
    "# print(\"NPV: \",R_MCP_signal_se[...,3])\n",
    "\n",
    "# R_MCP_signal_median = cp.median(R_MCP_signal, 2)\n",
    "# print(\"Sample median:\")\n",
    "# print(\"2-norm: \",R_MCP_signal_median[...,0])\n",
    "# print(\"infinity-norm: \",R_MCP_signal_median[...,1])\n",
    "# print(\"PPV: \",R_MCP_signal_median[...,2])\n",
    "# print(\"NPV: \",R_MCP_signal_median[...,3])\n",
    "# R_MCP_signal_mad = mad(R_MCP_signal.get(), 2)\n",
    "# print(\"Sample MAD:\")\n",
    "# print(\"2-norm: \",R_MCP_signal_mad[...,0])\n",
    "# print(\"infinity-norm: \",R_MCP_signal_mad[...,1])\n",
    "# print(\"PPV: \",R_MCP_signal_mad[...,2])\n",
    "# print(\"NPV: \",R_MCP_signal_mad[...,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
